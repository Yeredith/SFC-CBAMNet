{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as scio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de datos (data_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.mat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de la clase TrainsetFromFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TrainsetFromFolder(data.Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        super(TrainsetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza para manejar el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def __getitem__(self, index):\n",
    "        mat = scio.loadmat(self.image_filenames[index], verify_compressed_data_integrity=False)\n",
    "        input = mat['lr'].astype(np.float32)\n",
    "        label = mat['hr'].astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(input), torch.from_numpy(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga un archivo .mat y extrae dos variables:\n",
    "- lr (baja resolución) se almacena como input.\n",
    "- hr (alta resolución) se almacena como label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la clase ValsetFromFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ValsetFromFolder(data.Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        super(ValsetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está orientada al conjunto de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función __getitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def __getitem__(self, index):\n",
    "        mat = scio.loadmat(self.image_filenames[index])\n",
    "        input = mat['LR'].astype(np.float32).transpose(2, 0, 1)\n",
    "        label = mat['HR'].astype(np.float32).transpose(2, 0, 1)\n",
    "        \n",
    "        return torch.from_numpy(input).float(), torch.from_numpy(label).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los datos de validación y ajusta las matrices (input y label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de entrenamiento (option.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser(description=\"Super-Resolution\")\n",
    "        parser.add_argument(\"--upscale_factor\", default=4, type=int, help=\"super resolution upscale factor\")\n",
    "        parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "        parser.add_argument(\"--batchSize\", type=int, default=16, help=\"training batch size\")\n",
    "        parser.add_argument(\"--nEpochs\", type=int, default=100, help=\"maximum number of epochs to train\")\n",
    "        parser.add_argument(\"--show\", action=\"store_true\", help=\"show Tensorboard\")\n",
    "        parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n",
    "        parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda\")\n",
    "        parser.add_argument(\"--gpus\", default=\"0,1,2,3\", type=str, help=\"gpu ids (default: 0)\")\n",
    "        parser.add_argument(\"--threads\", type=int, default=12, help=\"number of threads for dataloader to use\")\n",
    "        parser.add_argument(\"--resume\", default=\"\", type=str, help=\"Path to checkpoint (default: none)\")\n",
    "        parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")\n",
    "        parser.add_argument(\"--datasetName\", default=\"CAVE\", type=str, help=\"data name\")\n",
    "        parser.add_argument('--n_module', type=int, default=5, help='number of modules')\n",
    "        parser.add_argument('--n_feats', type=int, default=32, help='number of feature maps')\n",
    "        parser.add_argument('--model_name', default='checkpoint_x4_n/model_4_epoch_100.pth', type=str, help='model name')\n",
    "        parser.add_argument('--method', default='SFCSR', type=str, help='method name')\n",
    "        \n",
    "        self.opt = parser.parse_args([])\n",
    "\n",
    "opt = Options().opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo (model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloque de Atención de Canal (ChannelAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, n_feats, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool3d(1)\n",
    "        self.fc1 = nn.Conv3d(n_feats, n_feats // ratio, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv3d(n_feats // ratio, n_feats, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usa un AdaptiveAvgPool3d con salida de tamaño (1, 1, 1), calculando el promedio de cada canal en un tensor de tres dimensiones\n",
    "- fc1 y fc2: son capas convolucionales de 3D que implementan una red de atención.\n",
    "- relu: aplicada después de fc1, introduce no linealidad.\n",
    "- sigmoid: aplicada al final, genera una máscara de atención con valores entre 0 y 1 para ponderar la importancia de cada canal.\n",
    "- avg_out y max_out, combinan la información promedio y máxima para obtener una máscara final de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloque CBAM simplificado (solo canal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, n_feats, ratio=8):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(n_feats, ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este bloque simplificado CBAM solo aplica atención de canal, usando el bloque ChannelAttention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase TwoCNN con CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TwoCNN(nn.Module):\n",
    "    def __init__(self, wn, n_feats=64): \n",
    "        super(TwoCNN, self).__init__()\n",
    "        self.body = wn(nn.Conv2d(n_feats, n_feats, kernel_size=(3,3), stride=1, padding=(1,1), bias=False))\n",
    "        self.cbam = CBAM(n_feats)\n",
    "        self.adjust_channels = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = self.cbam(out.unsqueeze(2)).squeeze(2) \n",
    "\n",
    "        # Ajusta canales si `x` y `out` no coinciden\n",
    "        if out.shape[1] != x.shape[1]:\n",
    "            self.adjust_channels = nn.Conv2d(out.shape[1], x.shape[1], kernel_size=1).to(out.device)\n",
    "            out = self.adjust_channels(out)\n",
    "        \n",
    "        out = torch.add(out, x)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una convolución Conv2d con un kernel de 3x3, stride de 1 y padding de 1, que mantiene la dimensión espacial de la entrada.\n",
    "- Un bloque de atención canal-espacial.\n",
    "- adjust_channels: se utiliza para ajustar dinámicamente la cantidad de canales en caso de que los canales de out y x no coincidan.\n",
    "- unsqueeze(2) añade una dimensión extra en el eje 2, que es necesario si CBAM espera datos en formato tridimensional, y luego squeeze(2) elimina esta dimensión después de aplicar la atención.\n",
    "- adjust_channels se define como una capa Conv2d con un kernel_size=1, que ajusta la cantidad de canales de out para que coincida con x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ThreeCNN con CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ThreeCNN(nn.Module):\n",
    "    def __init__(self, wn, n_feats=64):\n",
    "        super(ThreeCNN, self).__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        body_spatial = [wn(nn.Conv3d(n_feats, n_feats, kernel_size=(1,3,3), stride=1, padding=(0,1,1), bias=False)) for _ in range(2)]\n",
    "        body_spectral = [wn(nn.Conv3d(n_feats, n_feats, kernel_size=(3,1,1), stride=1, padding=(1,0,0), bias=False)) for _ in range(2)]\n",
    "        \n",
    "        self.body_spatial = nn.Sequential(*body_spatial)\n",
    "        self.body_spectral = nn.Sequential(*body_spectral)\n",
    "        self.cbam = CBAM(n_feats)\n",
    "        self.adjust_channels = None\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = x\n",
    "        for i in range(2):  \n",
    "            out_spatial = self.body_spatial[i](out)\n",
    "            out_spectral = self.body_spectral[i](out)\n",
    "            out = torch.add(out_spatial, out_spectral)\n",
    "            if i == 0:\n",
    "                out = self.act(out)\n",
    "        \n",
    "        out = self.cbam(out)\n",
    "        \n",
    "        # Ajusta canales de `out` a `x` dinámicamente si no coinciden\n",
    "        if out.shape[1] != x.shape[1]:\n",
    "            self.adjust_channels = nn.Conv3d(out.shape[1], x.shape[1], kernel_size=1).to(out.device)\n",
    "            out = self.adjust_channels(out)\n",
    "        \n",
    "        if out.shape == x.shape:\n",
    "            out = torch.add(out, x)\n",
    "        else:\n",
    "            print(f\"Dimension mismatch before final addition: out {out.shape}, x {x.shape}\")\n",
    "            return None\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- body_spatial realiza convoluciones con un kernel (1, 3, 3), enfocándose en las dimensiones espaciales sin alterar la dimensión espectral. \n",
    "- body_spectral usa un kernel (3, 1, 1), procesando las dimensiones espectrales sin modificar las dimensiones espaciales. \n",
    "- cbam es la capa de atención canal-espacial (CBAM) que refuerza las características más importantes en la salida conjunta de body_spatial y body_spectral.\n",
    "forward:\n",
    "- out_spatial procesa out usando la i-ésima capa de body_spatial.\n",
    "- out_spectral procesa out con la i-ésima capa de body_spectral.\n",
    "- Se suman los resultados out_spatial y out_spectral, y el resultado se almacena en out.\n",
    "En la primera iteración, out pasa por self.act, aplicando la función de activación ReLU.\n",
    "- Después de las iteraciones, out pasa por el bloque CBAM para recalibrar la atención de canal y espacial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase principal SFCSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SFCSR(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SFCSR, self).__init__()\n",
    "        scale = args.upscale_factor\n",
    "        n_feats = args.n_feats\n",
    "        self.n_module = args.n_module        \n",
    "                 \n",
    "        wn = lambda x: torch.nn.utils.weight_norm(x)\n",
    "    \n",
    "        self.gamma_X = nn.Parameter(torch.ones(self.n_module)) \n",
    "        self.gamma_Y = nn.Parameter(torch.ones(self.n_module)) \n",
    "        self.gamma_DFF = nn.Parameter(torch.ones(4))\n",
    "        self.gamma_FCF = nn.Parameter(torch.ones(2))\n",
    "        \n",
    "        ThreeHead = [wn(nn.Conv3d(1, n_feats, kernel_size=(1,3,3), stride=1, padding=(0,1,1), bias=False)),\n",
    "                     wn(nn.Conv3d(n_feats, n_feats, kernel_size=(3,1,1), stride=1, padding=(1,0,0), bias=False))]\n",
    "        self.ThreeHead = nn.Sequential(*ThreeHead)\n",
    "\n",
    "        TwoHead = [wn(nn.Conv2d(1, n_feats, kernel_size=(3,3),  stride=1, padding=(1,1), bias=False))]\n",
    "        self.TwoHead = nn.Sequential(*TwoHead)\n",
    "\n",
    "        TwoTail = []\n",
    "        if (scale & (scale - 1)) == 0: \n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                TwoTail.append(wn(nn.Conv2d(n_feats, n_feats*4, kernel_size=(3,3), stride=1, padding=(1,1), bias=False)))\n",
    "                TwoTail.append(nn.PixelShuffle(2))           \n",
    "        else:\n",
    "            TwoTail.append(wn(nn.Conv2d(n_feats, n_feats*9, kernel_size=(3,3), stride=1, padding=(1,1), bias=False)))\n",
    "            TwoTail.append(nn.PixelShuffle(3))  \n",
    "        TwoTail.append(wn(nn.Conv2d(n_feats, 1, kernel_size=(3,3),  stride=1, padding=(1,1), bias=False)))                                 \t    \t\n",
    "        self.TwoTail = nn.Sequential(*TwoTail)\n",
    "\n",
    "        self.twoCNN = nn.Sequential(*[TwoCNN(wn, n_feats) for _ in range(self.n_module)])\n",
    "        self.reduceD_Y = wn(nn.Conv2d(n_feats*self.n_module, n_feats, kernel_size=(1,1), stride=1, bias=False))                          \n",
    "        self.twofusion = wn(nn.Conv2d(n_feats, n_feats, kernel_size=(3,3),  stride=1, padding=(1,1), bias=False))\n",
    "\n",
    "        self.threeCNN = nn.Sequential(*[ThreeCNN(wn, n_feats) for _ in range(self.n_module)])\n",
    "        self.reduceD = nn.Sequential(*[wn(nn.Conv2d(n_feats*4, n_feats, kernel_size=(1,1), stride=1, bias=False)) for _ in range(self.n_module)])                              \n",
    "        self.reduceD_X = wn(nn.Conv3d(n_feats*self.n_module, n_feats, kernel_size=(1,1,1), stride=1, bias=False))\n",
    "        \n",
    "        threefusion = [wn(nn.Conv3d(n_feats, n_feats, kernel_size=(1,3,3), stride=1, padding=(0,1,1), bias=False)),\n",
    "                       wn(nn.Conv3d(n_feats, n_feats, kernel_size=(3,1,1), stride=1, padding=(1,0,0), bias=False))]\n",
    "        self.threefusion = nn.Sequential(*threefusion)\n",
    "\n",
    "        self.reduceD_DFF = wn(nn.Conv2d(n_feats*4, n_feats, kernel_size=(1,1), stride=1, bias=False))  \n",
    "        self.conv_DFF = wn(nn.Conv2d(n_feats, n_feats, kernel_size=(1,1), stride=1, bias=False)) \n",
    "        self.reduceD_FCF = wn(nn.Conv2d(n_feats*2, n_feats, kernel_size=(1,1), stride=1, bias=False))  \n",
    "        self.conv_FCF = wn(nn.Conv2d(n_feats, n_feats, kernel_size=(1,1), stride=1, bias=False))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red cuenta con dos partes, ThreeHead y TwoHead, que procesan las características de entrada en el dominio tridimensional y bidimensional, respectivamente.\n",
    "- La parte final de la red, TwoTail, se encarga de ajustar la salida para lograr el escalado deseado (scale).\n",
    "- twoCNN: Cada TwoCNN incluye una capa de convolución y un módulo de atención CBAM para resaltar características importantes.\n",
    "- twoFusion: Combina las características procesadas por los módulos twoCNN.\n",
    "- threeCNN: Módulo que procesa tanto información espacial como espectral en el dominio tridimensional.\n",
    "- threefusion: capa convolucional en 3D que combina la información procesada por los módulos threeCNN.\n",
    "\n",
    "La red incluye varias capas de reducción de dimensión (reduce) en 2D y 3D, que permiten ajustar el número de canales después de cada bloque de procesamiento\n",
    "Las capas conv_DFF y conv_FCF son capas convolucionales finales. Estas capas aseguran que las características importantes se preserven y se resalten en la imagen generada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward SFCSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def forward(self, x, y, localFeats, i):\n",
    "        x = x.unsqueeze(1)     \n",
    "        x = self.ThreeHead(x)    \n",
    "        skip_x = x         \n",
    "\n",
    "        y = y.unsqueeze(1)\n",
    "        y = self.TwoHead(y)\n",
    "        skip_y = y\n",
    "\n",
    "        channelX = []\n",
    "        channelY = []        \n",
    "\n",
    "        for j in range(self.n_module):        \n",
    "            x = self.threeCNN[j](x)    \n",
    "            x = torch.add(skip_x, x)          \n",
    "            channelX.append(self.gamma_X[j]*x)\n",
    "\n",
    "            y = self.twoCNN[j](y)           \n",
    "            y = torch.cat([y, x[:,:,0,:,:], x[:,:,1,:,:], x[:,:,2,:,:]],1)\n",
    "            y = self.reduceD[j](y)      \n",
    "            y = torch.add(skip_y, y)         \n",
    "            channelY.append(self.gamma_Y[j]*y) \n",
    "                              \n",
    "        x = torch.cat(channelX, 1)\n",
    "        x = self.reduceD_X(x)\n",
    "        x = self.threefusion(x)\n",
    "      \t                \n",
    "        y = torch.cat(channelY, 1)        \n",
    "        y = self.reduceD_Y(y) \n",
    "        y = self.twofusion(y)        \n",
    "     \n",
    "        y = torch.cat([self.gamma_DFF[0]*x[:,:,0,:,:], self.gamma_DFF[1]*x[:,:,1,:,:], self.gamma_DFF[2]*x[:,:,2,:,:], self.gamma_DFF[3]*y], 1)\n",
    "       \n",
    "        y = self.reduceD_DFF(y)  \n",
    "        y = self.conv_DFF(y)\n",
    "                       \n",
    "        if i == 0:\n",
    "            localFeats = y\n",
    "        else:\n",
    "            y = torch.cat([self.gamma_FCF[0]*y, self.gamma_FCF[1]*localFeats], 1) \n",
    "            y = self.reduceD_FCF(y)                    \n",
    "            y = self.conv_FCF(y) \n",
    "            localFeats = y  \n",
    "        y = torch.add(y, skip_y)\n",
    "        y = self.TwoTail(y) \n",
    "        y = y.squeeze(1)   \n",
    "                \n",
    "        return y, localFeats  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ThreeHead aplica convoluciones 3D a la información espectral y TwoHead aplica convoluciones 2D a la información espacial.\n",
    "- La red pasa por n_module (en este caso 5), donde cada bloque procesa a los datos aplicando convoluciones y atención a los datos.\n",
    "- Después del procesamiento en bloques, channelX y channelY se concatenan a lo largo del eje de canales, formando tensores más grandes con todas las características procesadas.\n",
    "- reduceD_X y threefusion aplican capas de reducción y fusión a los datos x, lo que combina las características espectrales. reduceD_Y y twofusion hacen lo mismo para los datos y.\n",
    "- Después se combina las tres primeras dimensiones espectrales de x y y.\n",
    "- reduceD_DFF y conv_DFF se utilizan para reducir la dimensionalidad de las características combinadas en y.\n",
    "- Se combinan las características de reduceD_FCF y conv_FCF y se encarga de reducir la dimensión de esta concatenación para crear un tensor ajustado.\n",
    "- Se suma y con skip_y en una conexión residual para preservar la información inicial de y, y TwoTail se aplica para llevar la salida a la resolución deseada, eliminando dimensiones redundantes con squeeze(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de funciones para el entrenamiento y validación (train.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para obtener el checkpoint más reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "    if len(checkpoints) == 0:\n",
    "        return None\n",
    "    checkpoints.sort(key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "    return os.path.join(checkpoint_dir, checkpoints[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de entrenamiento train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer, model, criterion, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Entrenando Época {epoch}/{opt.nEpochs}\", unit=\"batch\", leave=False) as pbar:\n",
    "        for iteration, batch in enumerate(train_loader, 1):\n",
    "            input, label = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            localFeats = []\n",
    "            for i in range(input.shape[1]):\n",
    "                if i == 0:\n",
    "                    x = input[:, 0:3, :, :]\n",
    "                    y = input[:, 0, :, :]\n",
    "                    new_label = label[:, 0, :, :]\n",
    "                elif i == input.shape[1] - 1:\n",
    "                    x = input[:, i-2:i+1, :, :]\n",
    "                    y = input[:, i, :, :]\n",
    "                    new_label = label[:, i, :, :]\n",
    "                else:\n",
    "                    x = input[:, i-1:i+2, :, :]\n",
    "                    y = input[:, i, :, :]\n",
    "                    new_label = label[:, i, :, :]\n",
    "\n",
    "                SR, localFeats = model(x, y, localFeats, i)\n",
    "                localFeats = localFeats.detach()\n",
    "\n",
    "                loss = criterion(SR, new_label)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "\n",
    "            if opt.show:\n",
    "                writer.add_scalar('Train/Loss', loss.item())\n",
    "\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizamos una pasada de entrenamiento para cada lote en el train_loader.\n",
    "- Después se calcula la pérdida de cada paso y se actualizan los pesos del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de validación val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def val(val_loader, model, epoch, device):\n",
    "    model.eval()\n",
    "    val_psnr = 0\n",
    "\n",
    "    with tqdm(total=len(val_loader), desc=f\"Validando Época {epoch}\", unit=\"batch\", leave=False) as pbar:\n",
    "        for iteration, batch in enumerate(val_loader, 1):\n",
    "            input, label = batch[0].to(device), batch[1].to(device)\n",
    "            SR = np.ones((label.shape[1], label.shape[2], label.shape[3])).astype(np.float32)\n",
    "\n",
    "            localFeats = []\n",
    "            for i in range(input.shape[1]):\n",
    "                if i == 0:\n",
    "                    x = input[:, 0:3, :, :]\n",
    "                    y = input[:, 0, :, :]\n",
    "                    new_label = label[:, 0, :, :]\n",
    "                elif i == input.shape[1] - 1:\n",
    "                    x = input[:, i-2:i+1, :, :]\n",
    "                    y = input[:, i, :, :]\n",
    "                    new_label = label[:, i, :, :]\n",
    "                else:\n",
    "                    x = input[:, i-1:i+2, :, :]\n",
    "                    y = input[:, i, :, :]\n",
    "                    new_label = label[:, i, :, :]\n",
    "\n",
    "                output, localFeats = model(x, y, localFeats, i)\n",
    "                SR[i, :, :] = output.cpu().data[0].numpy()\n",
    "\n",
    "            val_psnr += PSNR(SR, label.cpu().data[0].numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "    val_psnr = val_psnr / len(val_loader)\n",
    "    if opt.show:\n",
    "        writer.add_scalar('Val/PSNR', val_psnr, epoch)\n",
    "\n",
    "    return val_psnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúa el modelo en el conjunto de validación y calcula el PSNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para guardar gráficos y checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_plots(epoch):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(loss_values) + 1), loss_values, label='Pérdida (Loss)')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Pérdida por Época')\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path + f'loss_plot_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(psnr_values) + 1), psnr_values, label='PSNR')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('PSNR')\n",
    "    plt.title('PSNR por Época')\n",
    "    plt.legend()\n",
    "    plt.savefig(out_path + f'psnr_plot_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_checkpoint(model, epoch, optimizer):\n",
    "    model_out_path = os.path.join(checkpoint_dir, f\"model_{opt.upscale_factor}_epoch_{epoch}.pth\")\n",
    "    state = {\"epoch\": epoch, \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save(state, model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    if opt.show:\n",
    "        global writer\n",
    "        writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "    # Configurar el dispositivo\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Utilizando dispositivo: {device}\")\n",
    "\n",
    "    if opt.cuda and not torch.cuda.is_available():\n",
    "        raise Exception(\"No se encontró una GPU disponible, verifica tu configuración.\")\n",
    "\n",
    "    torch.manual_seed(opt.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    ## Cargar datasets\n",
    "    #train_set = TrainsetFromFolder('F:/HyperSSR/SFCSR_Modificado/Data/train/' + opt.datasetName + '/' + str(opt.upscale_factor) + '/')\n",
    "    #train_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\n",
    "\n",
    "    #val_set = ValsetFromFolder('F:/HyperSSR/SFCSR_Modificado/Data/test/' + opt.datasetName + '/' + str(opt.upscale_factor))\n",
    "    #val_loader = DataLoader(dataset=val_set, num_workers=opt.threads, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Definir el directorio base como el directorio actual del archivo\n",
    "    base_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    data_dir = os.path.join(base_dir, 'Data')\n",
    "\n",
    "    # Cargar datasets con rutas relativas\n",
    "    train_set = TrainsetFromFolder(os.path.join(data_dir, 'train', opt.datasetName, str(opt.upscale_factor)))\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\n",
    "\n",
    "    val_set = ValsetFromFolder(os.path.join(data_dir, 'test', opt.datasetName, str(opt.upscale_factor)))\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=opt.threads, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = SFCSR(opt).to(device)\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "\n",
    "    # Usar múltiples GPUs si están disponibles\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    print('# parameters:', sum(param.numel() for param in model.parameters()))\n",
    "\n",
    "    # Configurar el optimizador\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "    # Cargar checkpoint si existe\n",
    "    start_epoch = opt.start_epoch\n",
    "    checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if checkpoint:\n",
    "        print(f\"=> Cargando checkpoint '{checkpoint}'\")\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    else:\n",
    "        print(\"=> No se encontró ningún checkpoint, comenzando desde la época 1\")\n",
    "\n",
    "    # Scheduler para tasa de aprendizaje\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[35, 70, 105, 140, 175], gamma=0.5, last_epoch=start_epoch - 1)\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    for epoch in range(start_epoch, opt.nEpochs + 1):\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch = {epoch}, lr = {optimizer.param_groups[0]['lr']}\")\n",
    "        train_loss = train(train_loader, optimizer, model, criterion, epoch, device)\n",
    "        val_psnr = val(val_loader, model, epoch, device)\n",
    "\n",
    "        loss_values.append(train_loss)\n",
    "        psnr_values.append(val_psnr)\n",
    "\n",
    "        save_checkpoint(model, epoch, optimizer)\n",
    "        print(f\"Epoch [{epoch}/{opt.nEpochs}] - PSNR: {val_psnr:.3f}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            save_plots(epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
